{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdb08ae3-6e40-4e75-900d-da6aeb43993a",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fcf9da-864f-4d9e-b0da-5f995396dd17",
   "metadata": {},
   "source": [
    "# **Lab: Building Advanced Transformers**\n",
    "\n",
    "**Estimated time needed:  30 minutes**  \n",
    "\n",
    "In this lab, you will implement and experiment with advanced Transformer models using Keras. \n",
    "\n",
    "**Learning objectives:** \n",
    "\n",
    "By the end of this lab, you will: \n",
    "\n",
    "- Implement advanced Transformer models using Keras. \n",
    "\n",
    "- Apply Transformers to real-world sequential data tasks. \n",
    "\n",
    "- Build, train, and evaluate Transformer models. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f99b57-2c59-4862-8fac-9ff12df49057",
   "metadata": {},
   "source": [
    "## Step-by-Step Instructions: \n",
    "\n",
    "### Step 1: Import necessary libraries \n",
    "\n",
    "Before you start, you need to import the required libraries: TensorFlow and Keras. Keras is included within TensorFlow as `tensorflow.keras.`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f75f47fd-ad84-4ee2-bc2c-6311baafb7fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.Requirement already satisfied: tensorflow in d:\\programdata\\anaconda3\\envs\\myenv\\lib\\site-packages (2.19.0)\n",
      "Requirement already satisfied: pyarrow in d:\\programdata\\anaconda3\\envs\\myenv\\lib\\site-packages (19.0.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in d:\\programdata\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorflow) (2.2.2)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in d:\\programdata\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in d:\\programdata\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in d:\\programdata\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in d:\\programdata\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in d:\\programdata\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in d:\\programdata\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in d:\\programdata\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorflow) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in d:\\programdata\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorflow) (5.29.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in d:\\programdata\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in d:\\programdata\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorflow) (72.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in d:\\programdata\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in d:\\programdata\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorflow) (3.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in d:\\programdata\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in d:\\programdata\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorflow) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in d:\\programdata\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorflow) (1.71.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in d:\\programdata\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorflow) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in d:\\programdata\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorflow) (3.9.2)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in d:\\programdata\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in d:\\programdata\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorflow) (3.13.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in d:\\programdata\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorflow) (0.5.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in d:\\programdata\\anaconda3\\envs\\myenv\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in d:\\programdata\\anaconda3\\envs\\myenv\\lib\\site-packages (from keras>=3.5.0->tensorflow) (14.0.0)\n",
      "Requirement already satisfied: namex in d:\\programdata\\anaconda3\\envs\\myenv\\lib\\site-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in d:\\programdata\\anaconda3\\envs\\myenv\\lib\\site-packages (from keras>=3.5.0->tensorflow) (0.14.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\programdata\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\programdata\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\programdata\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\programdata\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
      "Requirement already satisfied: markdown>=2.6.8 in d:\\programdata\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in d:\\programdata\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in d:\\programdata\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in d:\\programdata\\anaconda3\\envs\\myenv\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in d:\\programdata\\anaconda3\\envs\\myenv\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in d:\\programdata\\anaconda3\\envs\\myenv\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in d:\\programdata\\anaconda3\\envs\\myenv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~lotly (d:\\ProgramData\\anaconda3\\envs\\myenv\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (d:\\ProgramData\\anaconda3\\envs\\myenv\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (d:\\ProgramData\\anaconda3\\envs\\myenv\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow pyarrow \n",
    "# %pip install pandas  \n",
    "# %pip install scikit-learn \n",
    "# %pip install matplotlib \n",
    "# %pip install requests\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7b493f8-9f56-4ea0-97cf-bb6c4f3ebfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import tensorflow as tf \n",
    "import requests\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "from tensorflow.keras.layers import Layer, Dense, LayerNormalization, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a6fc6c-1977-442d-a622-0794062257e1",
   "metadata": {},
   "source": [
    "####  Setup the Environment to generate synthetic stock price data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7a1644f-1983-4208-823e-1923cc2be243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic stock_prices.csv created and loaded.\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "# Create a synthetic stock price dataset\n",
    "np.random.seed(42)\n",
    "data_length = 2000  # Adjust data length as needed\n",
    "trend = np.linspace(100, 200, data_length)\n",
    "noise = np.random.normal(0, 2, data_length)\n",
    "synthetic_data = trend + noise\n",
    "\n",
    "# Create a DataFrame and save as 'stock_prices.csv'\n",
    "data = pd.DataFrame(synthetic_data, columns=['Close'])\n",
    "data.to_csv('stock_prices.csv', index=False)\n",
    "print(\"Synthetic stock_prices.csv created and loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b9f0a73-eb0e-4c0b-adb1-2f068a5f33d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (1899, 100, 1)\n",
      "Shape of Y: (1899,)\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset \n",
    "data = pd.read_csv('stock_prices.csv') \n",
    "data = data[['Close']].values \n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "data = scaler.fit_transform(data)\n",
    "\n",
    "# Prepare the data for training\n",
    "def create_dataset(data, time_step=1):\n",
    "    X, Y = [], []\n",
    "\n",
    "    for i in range(len(data)-time_step-1):\n",
    "        a = data[i:(i+time_step), 0]\n",
    "        X.append(a)\n",
    "        Y.append(data[i + time_step, 0])\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "time_step = 100\n",
    "X, Y = create_dataset(data, time_step)\n",
    "X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "\n",
    "print(\"Shape of X:\", X.shape) \n",
    "print(\"Shape of Y:\", Y.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b218abb-d401-4727-a317-2e381c2d1103",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "`tensorflow` is the main library for machine learning in Python.  \n",
    "\n",
    "`stock_prices.csv` is the data set that is loaded. \n",
    "\n",
    "`MinMaxScaler` method is used to normalize the data.  \n",
    "\n",
    "`create_dataset`method is used to prepare the data for training. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834533dc-d545-4225-abe5-bf6702a63b29",
   "metadata": {},
   "source": [
    "### Step 2: Implement Multi-Head Self-Attention \n",
    "\n",
    "Define the Multi-Head Self-Attention mechanism. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c17e005d-bb96-4e35-84f6-1b64ff95198f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(Layer): \n",
    "\n",
    "    def __init__(self, embed_dim, num_heads=8): \n",
    "        super(MultiHeadSelfAttention, self).__init__() \n",
    "        self.embed_dim = embed_dim \n",
    "        self.num_heads = num_heads \n",
    "        self.projection_dim = embed_dim // num_heads \n",
    "        self.query_dense = Dense(embed_dim) \n",
    "        self.key_dense = Dense(embed_dim) \n",
    "        self.value_dense = Dense(embed_dim) \n",
    "        self.combine_heads = Dense(embed_dim) \n",
    "\n",
    "\n",
    "    def attention(self, query, key, value): \n",
    "        score = tf.matmul(query, key, transpose_b=True) \n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32) \n",
    "        scaled_score = score / tf.math.sqrt(dim_key) \n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1) \n",
    "        output = tf.matmul(weights, value) \n",
    "        return output, weights \n",
    "\n",
    "    def split_heads(self, x, batch_size): \n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim)) \n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3]) \n",
    "\n",
    "    def call(self, inputs): \n",
    "        batch_size = tf.shape(inputs)[0] \n",
    "        query = self.query_dense(inputs) \n",
    "        key = self.key_dense(inputs) \n",
    "        value = self.value_dense(inputs) \n",
    "        query = self.split_heads(query, batch_size) \n",
    "        key = self.split_heads(key, batch_size) \n",
    "        value = self.split_heads(value, batch_size) \n",
    "        attention, _ = self.attention(query, key, value) \n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3]) \n",
    "        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim)) \n",
    "        output = self.combine_heads(concat_attention) \n",
    "        return output \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9559b2f1-6474-4761-b3e1-e8d463be0b80",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "- The MultiHeadSelfAttention layer implements the multi-head self-attention mechanism, which allows the model to focus on different parts of the input sequence simultaneously. \n",
    "\n",
    "- The attention parameter computes the attention scores and weighted sum of the values. \n",
    "\n",
    "- The split_heads parameter splits the input into multiple heads for parallel attention computation. \n",
    "\n",
    "- The call method applies the self-attention mechanism and combines the heads. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19eacfd8-cf47-49e2-b3d6-37f4a9f7fc91",
   "metadata": {},
   "source": [
    "### Step 3: Implement Transformer block \n",
    "\n",
    "Define the Transformer block. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d98b16c-1273-47db-a7c1-1b86c156f923",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(Layer): \n",
    "\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1): \n",
    "        super(TransformerBlock, self).__init__() \n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads) \n",
    "        self.ffn = tf.keras.Sequential([ \n",
    "            Dense(ff_dim, activation=\"relu\"), \n",
    "            Dense(embed_dim), \n",
    "        ]) \n",
    "\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6) \n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6) \n",
    "        self.dropout1 = Dropout(rate) \n",
    "        self.dropout2 = Dropout(rate) \n",
    "\n",
    "\n",
    "    def call(self, inputs, training): \n",
    "        attn_output = self.att(inputs) \n",
    "        attn_output = self.dropout1(attn_output, training=training) \n",
    "        out1 = self.layernorm1(inputs + attn_output) \n",
    "        ffn_output = self.ffn(out1) \n",
    "        ffn_output = self.dropout2(ffn_output, training=training) \n",
    "        return self.layernorm2(out1 + ffn_output) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ac1a9c-a6fd-426a-8150-7d73bbda0260",
   "metadata": {},
   "source": [
    "In the above code:\n",
    "\n",
    "- The TransformerBlock layer combines multi-head self-attention with a feed-forward neural network and normalization layers.  \n",
    "\n",
    "- Dropout is used to prevent overfitting. \n",
    "\n",
    "- The call method applies the self-attention, followed by the feedforward network with residual connections and layer normalization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44236ed7-6e90-4272-8ddb-0b23f162e801",
   "metadata": {},
   "source": [
    "### Step 4: Implement Encoder Layer \n",
    "\n",
    "Define the Encoder layer. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ae62188-09fc-4efa-be57-4ccdc7388d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(Layer): \n",
    "\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1): \n",
    "        super(EncoderLayer, self).__init__() \n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads) \n",
    "        self.ffn = tf.keras.Sequential([ \n",
    "            Dense(ff_dim, activation=\"relu\"), \n",
    "            Dense(embed_dim), \n",
    "        ]) \n",
    "\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6) \n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6) \n",
    "        self.dropout1 = Dropout(rate) \n",
    "        self.dropout2 = Dropout(rate) \n",
    "\n",
    " \n",
    "\n",
    "    def call(self, inputs, training): \n",
    "        attn_output = self.att(inputs) \n",
    "        attn_output = self.dropout1(attn_output, training=training) \n",
    "        out1 = self.layernorm1(inputs + attn_output) \n",
    "        ffn_output = self.ffn(out1) \n",
    "        ffn_output = self.dropout2(ffn_output, training=training) \n",
    "        return self.layernorm2(out1 + ffn_output) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c4a011-d544-467e-8dd4-5b785a226924",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "- The EncoderLayer is similar to the TransformerBlock but is a reusable layer in the Transformer architecture. \n",
    "\n",
    "- It consists of a MultiHeadSelfAttention mechanism followed by a feedforward neural network. \n",
    "\n",
    "- Both sub-layers have residual connections around them, and layer normalization is applied to the output of each sub-layer. \n",
    "\n",
    "- The call method applies the self-attention, followed by the feedforward network, with residual connections and layer normalization. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d804e1-689b-4876-be82-6a65a1381154",
   "metadata": {},
   "source": [
    "### Step 5: Implement Transformer encoder \n",
    "\n",
    "Define the Transformer Encoder. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4cc36bd-6bd8-4334-8571-d3ec5a17c69e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 100, 128)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow.keras.layers import Layer, Dense, LayerNormalization, Dropout \n",
    "\n",
    "class MultiHeadSelfAttention(Layer): \n",
    "    def __init__(self, embed_dim, num_heads=8): \n",
    "        super(MultiHeadSelfAttention, self).__init__() \n",
    "        self.embed_dim = embed_dim \n",
    "        self.num_heads = num_heads \n",
    "        self.projection_dim = embed_dim // num_heads \n",
    "        self.query_dense = Dense(embed_dim) \n",
    "        self.key_dense = Dense(embed_dim) \n",
    "        self.value_dense = Dense(embed_dim) \n",
    "        self.combine_heads = Dense(embed_dim) \n",
    " \n",
    "\n",
    "    def attention(self, query, key, value): \n",
    "        score = tf.matmul(query, key, transpose_b=True) \n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32) \n",
    "        scaled_score = score / tf.math.sqrt(dim_key) \n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1) \n",
    "        output = tf.matmul(weights, value) \n",
    "        return output, weights \n",
    "\n",
    "\n",
    "    def split_heads(self, x, batch_size): \n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim)) \n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3]) \n",
    "\n",
    "\n",
    "    def call(self, inputs): \n",
    "        batch_size = tf.shape(inputs)[0] \n",
    "        query = self.query_dense(inputs) \n",
    "        key = self.key_dense(inputs) \n",
    "        value = self.value_dense(inputs) \n",
    "        query = self.split_heads(query, batch_size) \n",
    "        key = self.split_heads(key, batch_size) \n",
    "        value = self.split_heads(value, batch_size) \n",
    "        attention, _ = self.attention(query, key, value) \n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3]) \n",
    "        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim)) \n",
    "        output = self.combine_heads(concat_attention) \n",
    "        return output \n",
    "\n",
    "class TransformerBlock(Layer): \n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1): \n",
    "        super(TransformerBlock, self).__init__() \n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads) \n",
    "        self.ffn = tf.keras.Sequential([ \n",
    "            Dense(ff_dim, activation=\"relu\"), \n",
    "            Dense(embed_dim), \n",
    "        ]) \n",
    "\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6) \n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6) \n",
    "        self.dropout1 = Dropout(rate) \n",
    "        self.dropout2 = Dropout(rate) \n",
    " \n",
    "\n",
    "    def call(self, inputs, training): \n",
    "        attn_output = self.att(inputs) \n",
    "        attn_output = self.dropout1(attn_output, training=training) \n",
    "        out1 = self.layernorm1(inputs + attn_output) \n",
    "        ffn_output = self.ffn(out1) \n",
    "        ffn_output = self.dropout2(ffn_output, training=training) \n",
    "        return self.layernorm2(out1 + ffn_output) \n",
    "\n",
    "class TransformerEncoder(Layer): \n",
    "    def __init__(self, num_layers, embed_dim, num_heads, ff_dim, rate=0.1): \n",
    "        super(TransformerEncoder, self).__init__() \n",
    "        self.num_layers = num_layers \n",
    "        self.embed_dim = embed_dim \n",
    "        self.enc_layers = [TransformerBlock(embed_dim, num_heads, ff_dim, rate) for _ in range(num_layers)] \n",
    "        self.dropout = Dropout(rate) \n",
    "\n",
    "    def call(self, inputs, training=False): \n",
    "        x = inputs \n",
    "        for i in range(self.num_layers): \n",
    "            x = self.enc_layers[i](x, training=training) \n",
    "        return x \n",
    "\n",
    "# Example usage \n",
    "embed_dim = 128 \n",
    "num_heads = 8 \n",
    "ff_dim = 512 \n",
    "num_layers = 4 \n",
    "\n",
    "transformer_encoder = TransformerEncoder(num_layers, embed_dim, num_heads, ff_dim) \n",
    "inputs = tf.random.uniform((1, 100, embed_dim)) \n",
    "outputs = transformer_encoder(inputs, training=False)  # Use keyword argument for 'training' \n",
    "print(outputs.shape)  # Should print (1, 100, 128) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58914cf7-70fa-4e3a-9a13-2c7ea907f91e",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "The TransformerEncoder is composed of multiple TransformerBlock layers, implementing the encoding part of the Transformer architecture. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade3a268-1398-489b-965b-63d7cb4f70b9",
   "metadata": {},
   "source": [
    "### Step 6: Build and Compile the Transformer model \n",
    "\n",
    "Integrate the Transformer Encoder into a complete model for sequential data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "973dc690-4c2f-4edf-aa69-63be850f3ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\ProgramData\\anaconda3\\envs\\myenv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:219: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_8\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_8\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_48 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_encoder_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">793,088</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoder</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12800</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_49 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,801</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_4 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_48 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │           \u001b[38;5;34m256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_encoder_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │       \u001b[38;5;34m793,088\u001b[0m │\n",
       "│ (\u001b[38;5;33mTransformerEncoder\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12800\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_49 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │        \u001b[38;5;34m12,801\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">806,145</span> (3.08 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m806,145\u001b[0m (3.08 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">806,145</span> (3.08 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m806,145\u001b[0m (3.08 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the necessary parameters \n",
    "\n",
    "embed_dim = 128 \n",
    "num_heads = 8 \n",
    "ff_dim = 512 \n",
    "num_layers = 4 \n",
    "\n",
    "# Define the Transformer Encoder \n",
    "transformer_encoder = TransformerEncoder(num_layers, embed_dim, num_heads, ff_dim) \n",
    "\n",
    "# Build the model \n",
    "input_shape = (X.shape[1], X.shape[2]) \n",
    "inputs = tf.keras.Input(shape=input_shape) \n",
    "\n",
    "# Project the inputs to the embed_dim \n",
    "x = tf.keras.layers.Dense(embed_dim)(inputs) \n",
    "encoder_outputs = transformer_encoder(x) \n",
    "flatten = tf.keras.layers.Flatten()(encoder_outputs) \n",
    "outputs = tf.keras.layers.Dense(1)(flatten) \n",
    "model = tf.keras.Model(inputs, outputs) \n",
    "\n",
    "# Compile the model \n",
    "model.compile(optimizer='adam', loss='mse') \n",
    "\n",
    "# Summary of the model \n",
    "model.summary() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fb9cf5-9372-4794-add8-5f2392838a23",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "- The Transformer Encoder model defines the necessary parameters, flattens the output, and ends with a dense layer to produce the final output.  \n",
    "\n",
    "- The model is then compiled with the Adam optimizer and mean squared error loss. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5978fb3-3a42-44f9-b146-68cad41ba794",
   "metadata": {},
   "source": [
    "### Step 7: Train the Transformer model \n",
    "\n",
    "Train the model on the prepared dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a65d7244-5b68-4ba7-9f94-27cb0022e768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 305ms/step - loss: 12.2983\n",
      "Epoch 2/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 295ms/step - loss: 0.2388\n",
      "Epoch 3/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 291ms/step - loss: 0.1879\n",
      "Epoch 4/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 292ms/step - loss: 0.1479\n",
      "Epoch 5/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 290ms/step - loss: 0.1495\n",
      "Epoch 6/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 293ms/step - loss: 0.1550\n",
      "Epoch 7/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 291ms/step - loss: 0.1396\n",
      "Epoch 8/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 292ms/step - loss: 0.1046\n",
      "Epoch 9/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 291ms/step - loss: 0.0907\n",
      "Epoch 10/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 290ms/step - loss: 0.1507\n",
      "Epoch 11/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 297ms/step - loss: 0.1114\n",
      "Epoch 12/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 310ms/step - loss: 0.0833\n",
      "Epoch 13/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 307ms/step - loss: 0.0812\n",
      "Epoch 14/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 309ms/step - loss: 0.1014\n",
      "Epoch 15/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 310ms/step - loss: 0.0729\n",
      "Epoch 16/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 308ms/step - loss: 0.0646\n",
      "Epoch 17/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 309ms/step - loss: 0.0665\n",
      "Epoch 18/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 306ms/step - loss: 0.0375\n",
      "Epoch 19/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 307ms/step - loss: 0.0339\n",
      "Epoch 20/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 309ms/step - loss: 0.0548\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x21c436c1e50>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(X, Y, epochs=20, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa04758-fc9b-4fb2-b00e-535326f274a6",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "The model is trained on the normalized stock price data for 20 epochs with a batch size of 32. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c73638-a697-414f-91b4-3cf1455015db",
   "metadata": {},
   "source": [
    "### Step 8: Evaluate and Make Predictions \n",
    "\n",
    "Evaluate the model's performance and make predictions on the dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14dc0b3a-758e-407e-b392-cd3b9a1c2fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 167ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make predictions \n",
    "predictions = model.predict(X) \n",
    "predictions = scaler.inverse_transform(predictions) \n",
    " \n",
    "\n",
    "# Plot the predictions \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "plt.plot(data, label='True Data') \n",
    "plt.plot(np.arange(time_step, time_step + len(predictions)), predictions, label='Predictions') \n",
    "plt.xlabel('Time') \n",
    "plt.ylabel('Stock Price') \n",
    "plt.legend() \n",
    "plt.show() \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7654ac42-8a22-4304-b257-2d344dcd360f",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "- The model's predictions are transformed back to the original scale using the inverse transform of the scaler. \n",
    "\n",
    "- The true data and predictions are plotted to visualize the model's performance. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9e038e-2bd0-48df-a073-317143f34a65",
   "metadata": {},
   "source": [
    "## Practice Exercises: \n",
    "\n",
    " ### Exercise 1: Add dropout to the Transformer model \n",
    "\n",
    " **Objective: Understand how to add dropout layers to the Transformer model to prevent overfitting.** \n",
    "\n",
    " Instructions: \n",
    "\n",
    "- Add a dropout layer after the Flatten layer in the model. \n",
    "\n",
    "- Set the dropout rate to 0.5. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658814d0-81f8-4e42-9196-f58a8ba73174",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your code here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b631728-90c3-4045-9a46-4981381b9f26",
   "metadata": {},
   "source": [
    "<details><summary>Click here to view the solution.</summary>\n",
    "\n",
    "```\n",
    "from tensorflow.keras.layers import Dropout \n",
    "\n",
    "  \n",
    "\n",
    "# Add a dropout layer after the Flatten layer \n",
    "\n",
    "flatten = tf.keras.layers.Flatten()(encoder_outputs) \n",
    "\n",
    "dropout = Dropout(0.5)(flatten) \n",
    "\n",
    "outputs = tf.keras.layers.Dense(1)(dropout) \n",
    "\n",
    "  \n",
    "\n",
    "# Build the model \n",
    "\n",
    "model = tf.keras.Model(inputs, outputs) \n",
    "\n",
    "  \n",
    "\n",
    "# Compile the model \n",
    "\n",
    "model.compile(optimizer='adam', loss='mse') \n",
    "\n",
    "  \n",
    "\n",
    "# Train the model \n",
    "\n",
    "model.fit(X, Y, epochs=20, batch_size=32) \n",
    "\n",
    "  \n",
    "\n",
    "# Evaluate the model \n",
    "\n",
    "loss = model.evaluate(X, Y) \n",
    "\n",
    "print(f'Test loss: {loss}') \n",
    "\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac4285e-8886-47e6-8946-e10493394914",
   "metadata": {},
   "source": [
    "### Exercise 2: Experiment with different batch sizes \n",
    "\n",
    "**Objective: Observe the impact of different batch sizes on model performance.** \n",
    "\n",
    " Instructions: \n",
    "\n",
    "- Train the model with a batch size of 16. \n",
    "\n",
    "- Train the model with a batch size of 64. \n",
    "\n",
    "- Compare the training time and performance. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960017cb-8c0e-4d60-9447-81f4be936add",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your code here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f752d7-fece-4267-9164-9a64bcae3911",
   "metadata": {},
   "source": [
    "<details><summary>Click here to view the solution.</summary>\n",
    "\n",
    "```\n",
    "# Train the model with batch size 16\n",
    "model.fit(X, Y, epochs=20, batch_size=16)\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X, Y)\n",
    "print(f'Test loss with batch size 16: {loss}')\n",
    "\n",
    "# Train the model with batch size 64\n",
    "model.fit(X, Y, epochs=20, batch_size=64)\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X, Y)\n",
    "print(f'Test loss with batch size 64: {loss}')\n",
    "\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359aec66-7a1c-4f1b-9be3-6a7f90905c3d",
   "metadata": {},
   "source": [
    "### Exercise 3: Use a different activation function \n",
    "\n",
    " **Objective: Understand how different activation functions impact the model performance.** \n",
    "\n",
    " Instructions: \n",
    "\n",
    "- Change the activation function of the Dense layer to `tanh`. \n",
    "\n",
    "- Train and evaluate the model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194078fc-6b2a-4543-8200-b3d6c9e71e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your code here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade7ca29-e106-4a76-97b6-5fd814a01a0e",
   "metadata": {},
   "source": [
    "<details><summary>Click here to view the solution.</summary>\n",
    "\n",
    "```\n",
    "# Change the activation function of the Dense layer to tanh\n",
    "outputs = tf.keras.layers.Dense(1, activation='tanh')(flatten)\n",
    "\n",
    "# Build the model\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, Y, epochs=20, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X, Y)\n",
    "print(f'Test loss with tanh activation: {loss}')\n",
    "\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b057d6-dab5-465e-bfc2-a2e1238297ad",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Congratulations on completing this lab! In this lab, you have built an advanced Transformer model using Keras and applied it to a time series forecasting task. You have learned how to define and implement multi-head self-attention, Transformer blocks, encoder layers, and integrate them into a complete Transformer model. By experimenting with different configurations and training the model, you can further improve its performance and apply it to various sequential data tasks. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d130ec79-66d4-4f13-9e97-f5d0508ec412",
   "metadata": {},
   "source": [
    "Copyright © IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "prev_pub_hash": "28ac4fd81c1d713f83dcd1cdf1d3383ad25ea92873288fe9e978e9a17b314709"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
